# AMPL Default Configuration YAML file
# Please Change configuration settings marked as 'REQUIRED USER MODIFICATION'
#   all other settings are 'OPTIONAL USER MODIFICATION'
#
#     Please see AMPL documentation on how to run AMPL


{% raw %}
# This Pipeline configuration YAML file uses jinja2 templating '{{ }}' to reuse variables,
#   please refer to jinja2 templating for advance use, in general we do not recommend changing it
#   Templating is only applied once, which means nested references of templating will not work!
{% endraw %}

# ////////// Step 0: Initialize Common Variables and Defaults ////////////

model:
  study_name: 'your_study_name' # REQUIRED USER MODIFICATION. !!Make it UNIQUE to avoid overwriting results!!
  target_variable: 'your_target_variable' # REQUIRED USER MODIFICATION.
  dataset_name: 'your_dataset_name' # REQUIRED USER MODIFICATION.
  model_name: '{{model.target_variable}}_{{model.dataset_name}}' # (Unique name used to specify which model the result files belong to)
  num_models: 1 # No. of top models to build by ampl, that'll be used later in ensemble step

  # the file paths below may need to be edited based your OS file structure
  # results_directory: './results'
  results_directory: './results_{{model.study_name}}'
  saved_models_directory: './results_{{model.study_name}}/saved_models/'
  plots_directory: './results_{{model.study_name}}/plots/'

  # All the AMPL journal will be outputted to a file journal_<target_variable>_<dataset_name>.json in the results dir
  # All the saved models will be outputted to <target_variable>_<dataset_name>_top_0_model.keras for NN and .json for DT
  # in the saved_models_directory location

# ////////////// Step 0.5: Data Pre-Processing //////////////

db: # File refers to SQLite3 database file and specifically its path
  data_file: './data/your_data.sqlite' # REQUIRED USER MODIFICATION , If using csv_file set this to null

data:
  # Only one input source is used, either read/write from Sqlite3 database file or CSV file or pre-split data
  # If both input sources are present CSV will take precedence

  data_table_name: 'your_data_table_name' # REQUIRED USER MODIFICATION. # set to the table name in your database.
  csv_file: null # Input data file in CSV file format with all the features (X) and target variable (y)
  pre_split: null
#    csv: null
#      train: 'path_to_training.csv'
#      test: 'path_to_test.csv'
#      val: 'path_to_test.csv'
#    sql: null
#      train: 'train_table_name'
#      test: 'test_table_name'
#      val: 'val_table_name'

  cols_to_enum:  null # REQUIRED USER MODIFICATION # String Columns to convert to Category type
#      - "enum_col_1"
#      - "enum_col_2"
#      - "enum_col_2"

  # All these percentages must add up to 1. This section will be ignored if using pre-split data.
  train_frac: 0.8 # percentage of data used for training.
  val_frac: 0.1 # percentage of data used for validation.
  test_frac: 0.1 # percentage of data used for testing.

# ////////////// Step 1: Feature Importance //////////////

feature_importance:
  feature_list:  # REQUIRED USER MODIFICATION # List of columns to include in study/run, don't include any target columns
    - 'col_1'
    - 'col_2'
    - 'col_3'
    - 'col_4'
    - 'col_5'
    - 'col_6'

  number_of_features: 6 # REQUIRED USER MODIFICATION # number of top important feature(s) to report

# ////////////// Steps 2 & 3: Neural Network/Decision Tree (XGBoost) Optuna and Model Build Settings //////////////

nn: # neural network build settings
  epochs: 1000 # No. of epochs to use in NN Model training
  patience: 50 # used for early stopping in NN Model training

dt: # decision tree - xgboost build settings
  early_stopping_rounds: 200
  
  #choose option 1 or 2 below when working with decision trees

  # Option 1
  loss: "reg:squarederror"
  eval_metric: 'rmse'

  # Option 2
  #loss: "reg:absoluteerror"
  #eval_metric: 'mae'

  verbosity: 1 # 0 (silent) - 3 (debug)

optuna: # Optuna general settings
  n_trials: 500 #
  direction: 'minimize' #'maximize' #
  parallel_percent: 0.9 # Percentage of cores to utilize for optuna between [0.1 - 0.9]
  # This will need a special constructor to be written to parse handling a function call
  #  being saved in a variable. you will also need to add the following library to the pipeline_config file
  # from pruners import MedianPruner
  pruner:
    !python/object:pruners.MedianPruner #

optuna_nn:
  best_trial_table_name: 'Best_Optuna_Trial_{{model.study_name}}_nn'
  top_trials_table_name: 'Top_Optuna_Trials_{{model.study_name}}_nn'
  trial_min_layers: 3 #
  trial_max_layers: 15 #
  max_neurons: 500 #
  min_lr: 0.001 # 1e-3 #
  max_lr: 0.1 #1e-1 #
  # If you would like to use the same batch_size for every trial (i.e. not vary it), set 'min_power' and 'max_power' to
  # the same value.
  # e.g. If you wanted to use Keras' default batch_size, which is 32, set both 'min_power' and 'max_power' to 5.
  # If you would like to tune the batch_size, set 'min_power' and 'max_power' to different values to set the range.
  min_power: 5  # power of 2 for batch_size, e.g. a value of 5 indicates batch_size 32 (2^5 = 32) [minimum value]
  max_power: 5  # power of 2 for batch_size, e.g. a value of 10 indicates batch_size 1024 (2^10 = 1024) [maximum value]
  trial_epochs: 20 #

  activations: #
    - relu
    - softmax
    - tanh
    - sigmoid
    - elu
    - selu

  optimizers: #
    - 'Adam'
    - 'SGD'
    - 'RMSprop'
    - 'Nadam'
    - 'Adadelta'
    - 'Adagrad'
    - 'Adamax'

  loss: 'mae' # The following loss functions are currently supported: 'mae', 'mape', 'mse', 'msle', 'log_cosh'
  initializer: #
    !python/object:tensorflow.keras.initializers.GlorotUniform
#    !python/object:tensorflow.keras.initializers.RandomUniform
#    kwargs:
#      minval: 0.0
#      maxval: 1.0

optuna_dt: # Optuna - xgboost hyper-parameter optimization settings
  best_trial_table_name: 'Best_Optuna_Trial_{{model.study_name}}_dt'
  top_trials_table_name: 'Top_Optuna_Trials_{{model.study_name}}_dt'

  early_stopping_rounds: 200
  observation_key: 'validation_0-rmse'
  sampler: !python/object:samplers.TPESampler
  multivariate: false

  ranges: # Ranges [min, max]
    n_estimators: [100, 1000]
    max_depth: [4, 10]
    learning_rate: [0.005, 0.05]
    col_sample_by_tree: [0.2, 0.6]
    subsample: [0.4, 0.8]
    alpha: [0.01, 10.0]
    lambda: [.00000001, 10.0]
    gamma: [.00000001, 10.0]
    min_child_weight: [10, 1000]

# ////////////// Step 4: Model Evaluation //////////////
#################### Options ####################
# There are no options available for evaluation step
eval_nn: null

eval_dt: null

# ////////////// Step 5: Model Ensemble //////////////
#################### Options ####################
ensemble_nn: # Currently NN is only supported
  # ensemble to produce
  # '1A' = NN simple avg.
  # '1M' = NN median
  # '1WA' = NN weighted avg.

  ensemble_mode: '1A' # str

ensemble_dt: # DT ensemble not supported yet
  # ensemble to produce
  # '1A' = NN simple avg.
  # '1M' = NN median
  # '1WA' = NN weighted avg.

  ensemble_mode: '1A' # str

# ////////////// Optional Step: Inferencing //////////////
#################### Options ####################
# If you have previously trained a model with AMPL, you have the ability to use that saved model to make inferences on
# new unseen data within the same ranges as the data you originally used to train that saved model. Be sure that the
# original dataset that was used to train the model is being referenced in the 'data' section above so that
# the correct metadata is saved. This ensures that your new data is pre-processed using the same schemes as the
# original data. Also be sure that the saved model is in your run directory in the 'saved_models' folder, then edit this
# section to point to the new data you would like to make inferences on.

infer: # Make predictions on new unseen data using existing model
  infer_dataset_name: null # Unique name used to specify which dataset the model inference output file belongs to

  infer_db: null # IMPORTANT: If using a csv file set this to null
#    infer_data_db: './data/your_data.sqlite' # Input data file in SQLite file format
#    infer_data_table_name: 'your_table_name' # set to the table name in your new database
  infer_csv: null # IMPORTANT: If using a db file set this to null
#    infer_data_csv: './data/your_data.csv' # Input data file in CSV file format

